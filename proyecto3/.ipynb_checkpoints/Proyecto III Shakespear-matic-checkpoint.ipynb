{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto II: Shakespear-Matic\n",
    "\n",
    "Debido a la falta de producción literaria de calidad (subjetivamente medida por el profesor Alexander), el profesor  Alexander ha decidio explorar la generación automatica de texto, para ver si de esa forma se puede lograr mejores obras contemporaneas. Debido a que los estudiantes del curso de NLP (Natural Language Processing) ya son expertos en la generación automatica de texto, el profesor ha decidio generar el siguiente reto para evaluar sus conocimientos y la calidad de estas herramientas  en la producción literaria.\n",
    "\n",
    "Para esta tarea el profesor quiere que los estudiantes entrenen diferentes sistemas para la generación de tres estilos literarios diferentes, ente los estilos que se desean observar los estudiantes pueden escoger entre: Novelas clásicas, cuentos de niños, poesia, letras de canciones (por favor NO regueton), obras de realismo mágico, entre otras.\n",
    "\n",
    "Para el desarrollo de esta tarea el profesor Alexander pide que se diseñen sistemas de generación automatica basada en palabras, para lo cual deben implementar en el pipeline:\n",
    "\n",
    "1. Adquisición de datos.\n",
    "2. Generación de vectors de palabras usando word2vec.\n",
    "3. Entrenamiento de los sistemas para los estilos literarios escogidos.\n",
    "4. Predicción del texto:\n",
    "    * Si la codificación se hizo con onehot encoding para la salida, el sistema produce de salida la siguiente palabra a generar.\n",
    "    * Si la codificación de salida se hizo con word2vec se debe implementar una busqueda de la palbra más cercana a la codificación generada por la salida de la red.\n",
    "      \n",
    "Tengan en cuenta que la relación entre la longitud del corpus y las palabras del vocabulario debe ser adecuada, para poder tener buenos resultados. El texto generado debe tener tambien en cuenta la puntuación.\n",
    "\n",
    "AL finalizar el proyecto, el profesor Alexander quiere que los estudiantes contesten las siguientes preguntas:\n",
    "\n",
    "1. ¿Qué pueden observar de los resusltados de los sistemas de generacion automatica para los tres estilos literarios diferentes? ¿Porqué considera que los resultados son de está forma?\n",
    "\n",
    "2. ¿Qué tamaño de N escogierón para la codificación de las palabras en vectores? ¿?Qué sucede si escogen un valor diferente de N?\n",
    "\n",
    "3. ¿Qué sucede si se cambia el número de palbras anteriores utilizados para predecir la palabra siguiiente en el modelo?\n",
    "\n",
    "4. ¿Qué ventajas y desventajas tienen las diferentes formas de codificar la salida - one hot encoding y word2vec?\n",
    "\n",
    "5. ¿Qué forma de codificación de la salida escogierón al final? ¿Porqué?\n",
    "\n",
    "6. ¿Qué pueden concluir del proyecto? ¿Cómo se pueden mejorar los resultados?\n",
    "\n",
    "Por favor al desarrollar el proyecto comentar cada bloque de codigo con un analisis de lo que esperan lograr y un análisis de los resultados preliminares. El proyecto se debe entregar a más tardar el **Domingo 2 de Mayo del 2021 a las 11:59 p.m. hora Bogotá**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para leer y procesar el archivo en texto plano.\n",
    "\n",
    "def get_file_data(fname,file = True, stop_word_removal='no'):\n",
    "    file_contents = []\n",
    "    if file:\n",
    "        with open(fname) as f:\n",
    "            file_contents = f.read()\n",
    "    text = []\n",
    "    for val in file_contents.split('.'):\n",
    "        val = re.sub(r'[,¡!¿?;-]+','.',val)\n",
    "        val = re.sub(r'á','a',val)\n",
    "        val = re.sub(r'é','e',val)\n",
    "        val = re.sub(r'í','i',val)\n",
    "        val = re.sub(r'ó','o',val)\n",
    "        val = re.sub(r'ú','u',val)\n",
    "        val = re.sub(r'Á','A',val)\n",
    "        val = re.sub(r'É','E',val)\n",
    "        val = re.sub(r'Í','I',val)\n",
    "        val = re.sub(r'Ó','O',val)\n",
    "        val = re.sub(r'Ú','U',val)\n",
    "        val = re.sub(r'ñ','n',val)\n",
    "        val = re.sub(r'Ñ','N',val)\n",
    "        val = re.sub(r'[^a-zA-Z0-9 .]','',val)\n",
    "        #val = re.sub('[0-9]+',\"numero\",val)\n",
    "        val = re.sub('[0-9]+',\"\",val)\n",
    "        #sent = re.findall(\"[A-Za-z]+\", val)\n",
    "        sent = re.findall(\"[A-Za-z ]+\", val)\n",
    "        line = ''\n",
    "        \n",
    "        #print(sent)\n",
    "        \n",
    "        for words in sent:\n",
    "            #print(list(words))\n",
    "            #print(words)\n",
    "            \n",
    "            if stop_word_removal == 'yes': \n",
    "                if len(words) > 1 and words not in stop_words:\n",
    "                    line = line + ' ' + words\n",
    "            else:\n",
    "                \n",
    "                if len(words) > 1 :\n",
    "                    line = line + ' ' + words\n",
    "                    text.append(line.lower())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Función para obtener un Vocabulario en función del texto procesado\n",
    "\n",
    "def generate_dictionary_data(text):\n",
    "    word_to_index= dict()\n",
    "    index_to_word = dict()\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    vocab_size = 0\n",
    "    \n",
    "    for row in text:\n",
    "        word = row.lower()\n",
    "        corpus.append(word)\n",
    "        if word_to_index.get(word) == None:\n",
    "            word_to_index.update ( {word : count})\n",
    "            index_to_word.update ( {count : word })\n",
    "            count  += 1\n",
    "    vocab_size = len(word_to_index)\n",
    "    length_of_corpus = len(corpus)\n",
    "    \n",
    "    return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus\n",
    "\n",
    "# Función para generar representaciones one hot de los vectores target y del corpus\n",
    "\n",
    "def get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index,window_size):\n",
    "    \n",
    "    #Create an array of size = vocab_size filled with zeros\n",
    "    trgt_word_vector = np.zeros(vocab_size)\n",
    "    \n",
    "    #Get the index of the target_word according to the dictionary word_to_index. \n",
    "    index_of_word_dictionary = word_to_index.get(target_word) \n",
    "    \n",
    "    #Set the index to 1\n",
    "    trgt_word_vector[index_of_word_dictionary] = 1\n",
    "    \n",
    "    #Repeat same steps for context_words but in a loop\n",
    "    ctxt_word_vector = np.zeros((window_size,vocab_size))\n",
    "    \n",
    "    \n",
    "    row = 0\n",
    "    for word in context_words:\n",
    "        index_of_word_dictionary = word_to_index.get(word) \n",
    "        ctxt_word_vector[row,index_of_word_dictionary] = 1\n",
    "        row = row + 1\n",
    "    \n",
    "        \n",
    "#     print(\" HOT ENCODING \")\n",
    "#     print(\"word \\n\",trgt_word_vector)\n",
    "#     print(\"context \\n\",ctxt_word_vector)\n",
    "    return trgt_word_vector,ctxt_word_vector\n",
    "\n",
    "# Función para generar los datos de entrenamiento para la red neuronal que representa el modelo word2vec\n",
    "\n",
    "def generate_training_data(corpus,window_size,vocab_size,word_to_index,length_of_corpus,sample=None):\n",
    "\n",
    "    training_X = [] \n",
    "    training_y = []\n",
    "    \n",
    "    training_sample_words =  []\n",
    "    for i,word in enumerate(corpus):\n",
    "\n",
    "        index_target_word = i\n",
    "        target_word = word\n",
    "        context_words = []\n",
    "        \n",
    "#         print(\"i ---> \",i)\n",
    "#         print(\"word ---> \",word)\n",
    "        \n",
    "        if i >= window_size:\n",
    "            \n",
    "            before_target_word_index = index_target_word - 1\n",
    "            for x in range(before_target_word_index, before_target_word_index - window_size , -1):\n",
    "                if x >=0:\n",
    "                    context_words.extend([corpus[x]])\n",
    "            \n",
    "            context_words.reverse()\n",
    "            #print(\"context \\n\", context_words)\n",
    "            \n",
    "            trgt_word_vector,ctxt_word_vector = get_one_hot_vectors(target_word,context_words,vocab_size,word_to_index,window_size)\n",
    "#             print('tgt ',trgt_word_vector.shape)\n",
    "#             print('ctxt ',ctxt_word_vector.shape)\n",
    "            \n",
    "            training_X.append(ctxt_word_vector)\n",
    "            \n",
    "            training_y.append(trgt_word_vector)\n",
    "#         print(\" \") \n",
    "        \n",
    "        if sample is not None:\n",
    "            training_sample_words.append([target_word,context_words])   \n",
    "        \n",
    "    return np.array(training_X),np.array(training_y),training_sample_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
